# Technical Specification for MetadataExtractorAgent Implementation

*Generated on August 07, 2025, at 10:15 AM EDT*

## 1. Overview

The `MetadataExtractorAgent` is an autonomous component in the AI-based code analyzer and modernization application, responsible for extracting metadata from Abstract Syntax Trees (ASTs) generated by the `ParserAgent`. It computes metrics (e.g., lines of code, cyclomatic complexity, comment ratio) and identifies dependencies, preparing data for AI-driven analysis (e.g., by `CodeSmellAgent` in Phase 3). The agent operates within an agentic architecture coordinated by the `MCPOrchestratorAgent` using Multi-Agent Constraint Programming (MCP).

### 1.1 Responsibilities
- **Metadata Extraction**: Compute code metrics from ASTs, including lines of code, cyclomatic complexity, and comment ratio.
- **Dependency Analysis**: Identify dependencies (e.g., imported modules in Python, `require` statements in JavaScript).
- **Constraint Compliance**: Adhere to MCP constraints (e.g., resource limits, processing deadlines).
- **Event Publishing**: Notify downstream agents (e.g., `CodeSmellAgent`, `StorageAgent`) via a message bus (e.g., Kafka).
- **Error Handling**: Handle invalid ASTs or processing errors, logging issues for compliance.

### 1.2 Constraints
- **Resource Constraints**:
  - Maximum 2GB RAM per extraction task.
  - Maximum 1 CPU core per task.
- **Temporal Constraints**:
  - Process metadata for 100K lines of code within 30 seconds (MVP target).
- **Dependency Constraints**:
  - Requires AST from `ParserAgent` via `ASTGenerated` event.
  - Must complete before AI analysis by `CodeSmellAgent` (Phase 3).
- **Security Constraints**:
  - Ensure no sensitive data (e.g., hardcoded secrets) is included in metadata.
  - Log all extraction events for auditing.

### 1.3 Integration Points
- **Input**: Receives AST and file metadata from `ParserAgent` via `ASTGenerated` event.
- **Output**: Produces metadata (metrics, dependencies) as a `MetadataExtracted` event.
- **Coordination**: Interacts with `MCPOrchestratorAgent` for task assignment and constraint enforcement.
- **Storage**: Sends metadata to `StorageAgent` for persistence in MongoDB.

## 2. Data Schema

The `MetadataExtractorAgent` uses the following schemas, extending the BRD’s schema for metadata-specific data.

### 2.1 Input Schema
Received via the `ASTGenerated` event:
```json
{
  "event_id": "UUID",
  "file_id": "UUID",
  "repository_id": "UUID",
  "language": "string",
  "ast": "json",
  "constraints": {
    "resource_limit": { "cpu": "integer", "memory": "string" },
    "deadline": "timestamp",
    "priority": "enum(high, medium, low)"
  }
}
```

### 2.2 Output Schema
Published as the `MetadataExtracted` event:
```json
{
  "event_id": "UUID",
  "file_id": "UUID",
  "repository_id": "UUID",
  "metadata": {
    "language": "string",
    "metrics": {
      "lines_of_code": "integer",
      "cyclomatic_complexity": "integer",
      "comment_ratio": "float"
    },
    "dependencies": ["string"]
  },
  "constraints_satisfied": "boolean",
  "constraint_violations": ["string"],
  "error_message": "string | null",
  "timestamp": "timestamp"
}
```

### 2.3 Storage Schema
Stored via `StorageAgent` in MongoDB:
```json
{
  "metadata_id": "UUID",
  "file_id": "UUID",
  "language": "string",
  "metrics": {
    "lines_of_code": "integer",
    "cyclomatic_complexity": "integer",
    "comment_ratio": "float"
  },
  "dependencies": ["string"],
  "created_at": "timestamp",
  "constraints_satisfied": "boolean",
  "constraint_violations": ["string"]
}
```

## 3. Endpoints

The `MetadataExtractorAgent` exposes internal endpoints for communication within the agentic system, using Kafka topics for message passing.

### 3.1 Extract Metadata
- **Topic**: `metadata.extract`
- **Request**:
  ```typescript
  interface ExtractMetadataRequest {
    event_id: string;
    file_id: string;
    repository_id: string;
    language: string;
    ast: object;
    constraints: {
      resource_limit?: { cpu: number; memory: string };
      deadline?: string;
      priority: "high" | "medium" | "low";
    };
  }
  ```
- **Response** (Published to `metadata.result`):
  ```typescript
  interface ExtractMetadataResponse {
    event_id: string;
    file_id: string;
    repository_id: string;
    metadata: {
      language: string;
      metrics: {
        lines_of_code: number;
        cyclomatic_complexity: number;
        comment_ratio: number;
      };
      dependencies: string[];
    };
    constraints_satisfied: boolean;
    constraint_violations?: string[];
    error_message?: string;
    timestamp: string;
  }
  ```

### 3.2 Health Check
- **Topic**: `metadata.health`
- **Request**:
  ```typescript
  interface HealthCheckRequest {
    agent_id: string;
  }
  ```
- **Response**:
  ```typescript
  interface HealthCheckResponse {
    agent_id: string;
    status: "healthy" | "unhealthy";
    resource_usage: { cpu: number; memory: string };
    active_tasks: number;
  }
  ```

## 4. Mini Component Diagram

The following diagram highlights the `MetadataExtractorAgent`’s interactions within the system, focusing on its role in the agentic architecture.

```mermaid
classDiagram
    subgraph CoreAgents
        MetadataExtractorAgent
        MCPOrchestratorAgent
        ParserAgent
        StorageAgent
        CodeSmellAgent
    end

    class MetadataExtractorAgent {
        +extractMetadata()
        +computeMetrics()
    }
    class MCPOrchestratorAgent {
        +solveConstraints()
        +coordinateAgents()
    }
    class ParserAgent {
        +parseCode()
        +generateAST()
    }
    class StorageAgent {
        +storeData()
        +redactSensitiveData()
    }
    class CodeSmellAgent {
        +detectCodeSmells()
    }

    MCPOrchestratorAgent --> MetadataExtractorAgent : Coordinates
    ParserAgent --> MetadataExtractorAgent : Sends ASTGenerated
    MetadataExtractorAgent --> CodeSmellAgent : Sends MetadataExtracted (Phase 3)
    MetadataExtractorAgent --> StorageAgent : Stores Metadata
    StorageAgent --> MongoDB : Stores
    MetadataExtractorAgent --> Kafka : Publishes to

    note for CodeSmellAgent "Phase 3"
```

**Diagram Explanation**:
- **Grouping**: Agents are grouped in a `CoreAgents` subgraph for brevity.
- **Focus**: Emphasizes `MetadataExtractorAgent`’s interactions:
  - Receives `ASTGenerated` from `ParserAgent`.
  - Publishes `MetadataExtracted` to `CodeSmellAgent` (Phase 3).
  - Stores metadata via `StorageAgent`.
  - Coordinated by `MCPOrchestratorAgent`.
- **External Systems**: Shows MongoDB for storage and Kafka for event publishing.
- **Phase Annotation**: Notes `CodeSmellAgent` as a Phase 3 component.

## 5. Python Implementation

The `MetadataExtractorAgent` is implemented in Python, using libraries like `astroid` (Python AST analysis) and `esprima` (JavaScript/TypeScript). It runs as a worker process, consuming Kafka messages and publishing results.

### 5.1 Dependencies
```bash
pip install kafka-python astroid esprima psutil
```

### 5.2 Core Implementation
```python
import json
import time
import uuid
from kafka import KafkaConsumer, KafkaProducer
from astroid import nodes
import esprima
import psutil
from datetime import datetime, timedelta
from typing import Dict, Optional, List

class MetadataExtractorAgent:
    def __init__(self, kafka_bootstrap_servers: str):
        self.producer = KafkaProducer(bootstrap_servers=kafka_bootstrap_servers)
        self.consumer = KafkaConsumer(
            'metadata.extract',
            bootstrap_servers=kafka_bootstrap_servers,
            group_id='metadata_agent_group'
        )

    def compute_metrics(self, language: str, ast: Dict, content: str) -> Dict:
        """Compute code metrics from AST."""
        metrics = {
            'lines_of_code': 0,
            'cyclomatic_complexity': 1,  # Default base complexity
            'comment_ratio': 0.0
        }

        if language == 'Python':
            lines = content.split('\n')
            metrics['lines_of_code'] = len(lines)
            # Simplified cyclomatic complexity (count decision points)
            complexity = 1
            for node in ast.walk():
                if isinstance(node, (nodes.If, nodes.For, nodes.While, nodes.TryExcept)):
                    complexity += 1
            metrics['cyclomatic_complexity'] = complexity
            comments = sum(1 for line in lines if line.strip().startswith('#'))
            metrics['comment_ratio'] = comments / max(1, len(lines))

        elif language in ['JavaScript', 'TypeScript']:
            lines = content.split('\n')
            metrics['lines_of_code'] = len(lines)
            # Simplified complexity (count branches)
            def count_nodes(node):
                count = 0
                if isinstance(node, dict):
                    if node.get('type') in ['IfStatement', 'ForStatement', 'WhileStatement']:
                        count += 1
                    for value in node.values():
                        count += count_nodes(value)
                elif isinstance(node, list):
                    for item in node:
                        count += count_nodes(item)
                return count
            metrics['cyclomatic_complexity'] = count_nodes(ast) + 1
            comments = sum(1 for line in lines if line.strip().startswith('//') or '/*' in line)
            metrics['comment_ratio'] = comments / max(1, len(lines))

        return metrics

    def extract_dependencies(self, language: str, ast: Dict) -> List[str]:
        """Extract dependencies from AST."""
        dependencies = []
        if language == 'Python':
            for node in ast.walk():
                if isinstance(node, nodes.Import):
                    dependencies.extend(n.name[0] for n in node.names)
                elif isinstance(node, nodes.ImportFrom):
                    dependencies.append(node.modname)
        elif language in ['JavaScript', 'TypeScript']:
            def traverse(node):
                if isinstance(node, dict) and node.get('type') == 'CallExpression' and node.get('callee', {}).get('name') == 'require':
                    if node.get('arguments', []) and node['arguments'][0].get('type') == 'Literal':
                        dependencies.append(node['arguments'][0]['value'])
                for value in node.values():
                    if isinstance(value, (dict, list)):
                        traverse(value)
                if isinstance(node, list):
                    for item in node:
                        traverse(item)
            traverse(ast)
        return list(set(dependencies))

    def check_constraints(self, constraints: Dict, start_time: float) -> tuple[bool, List[str]]:
        """Validate MCP constraints."""
        violations = []
        satisfied = True

        # Resource constraints
        process = psutil.Process()
        memory_info = process.memory_info()
        cpu_percent = process.cpu_percent(interval=0.1)
        if constraints.get('resource_limit'):
            if memory_info.rss > int(constraints['resource_limit']['memory'].replace('GB', '')) * 1e9:
                violations.append("Memory limit exceeded")
                satisfied = False
            if cpu_percent / psutil.cpu_count() > constraints['resource_limit']['cpu'] * 100:
                violations.append("CPU limit exceeded")
                satisfied = False

        # Temporal constraints
        if constraints.get('deadline'):
            deadline = datetime.fromisoformat(constraints['deadline'])
            if datetime.now() > deadline:
                violations.append("Deadline exceeded")
                satisfied = False

        return satisfied, violations

    def process_ast(self, message: Dict):
        """Process AST and publish metadata."""
        event_id = str(uuid.uuid4())
        file_id = message['file_id']
        repository_id = message['repository_id']
        language = message['language']
        ast = message['ast']
        constraints = message.get('constraints', {})

        start_time = time.time()
        try:
            # Fetch content for line-based metrics (simplified, assuming content available)
            content = "mock_content"  # Replace with actual content fetch if needed

            # Compute metrics
            metrics = self.compute_metrics(language, ast, content)

            # Extract dependencies
            dependencies = self.extract_dependencies(language, ast)

            # Check constraints
            constraints_satisfied, constraint_violations = self.check_constraints(constraints, start_time)

            # Prepare response
            response = {
                'event_id': event_id,
                'file_id': file_id,
                'repository_id': repository_id,
                'metadata': {
                    'language': language,
                    'metrics': metrics,
                    'dependencies': dependencies
                },
                'constraints_satisfied': constraints_satisfied,
                'constraint_violations': constraint_violations,
                'error_message': None,
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            response = {
                'event_id': event_id,
                'file_id': file_id,
                'repository_id': repository_id,
                'metadata': None,
                'constraints_satisfied': False,
                'constraint_violations': ["Extraction error"],
                'error_message': str(e),
                'timestamp': datetime.now().isoformat()
            }

        # Publish result
        self.producer.send('metadata.result', json.dumps(response).encode('utf-8'))

    def run(self):
        """Main loop to consume and process messages."""
        for message in self.consumer:
            try:
                msg = json.loads(message.value.decode('utf-8'))
                self.process_ast(msg)
            except Exception as e:
                print(f"Error processing message: {str(e)}")

    def health_check(self, agent_id: str) -> Dict:
        """Return health status and resource usage."""
        process = psutil.Process()
        return {
            'agent_id': agent_id,
            'status': 'healthy',
            'resource_usage': {
                'cpu': process.cpu_percent(interval=0.1) / psutil.cpu_count(),
                'memory': f"{process.memory_info().rss / 1e9}GB"
            },
            'active_tasks': len(self.consumer.assignment())
        }
```

### 5.3 Configuration
- **Kafka Topics**:
  - Subscribe to: `metadata.extract`
  - Publish to: `metadata.result`, `metadata.health`
- **Environment Variables**:
  - `KAFKA_BOOTSTRAP_SERVERS`: Kafka server address (e.g., `localhost:9092`).
- **Constraints Example**:
  ```json
  {
    "resource_limit": { "cpu": 1, "memory": "2GB" },
    "deadline": "2025-08-07T10:20:00Z",
    "priority": "medium"
  }
  ```

## 6. Error Handling and Logging
- **Error Handling**:
  - Catch extraction exceptions and include in `error_message`.
  - Validate constraints and report violations in `constraint_violations`.
  - Retry transient failures (e.g., Kafka publish errors) up to 3 times.
- **Logging**:
  - Log events to `AuditLog` via `ComplianceAgent`:
    ```json
    {
      "event_id": "UUID",
      "type": "metadata_extract",
      "timestamp": "timestamp",
      "user_id": "UUID",
      "details": "Extracted metadata for file {file_id}",
      "compliance_status": "compliant",
      "constraint_violations": []
    }
    ```
  - Use Python’s `logging` module:
    ```python
    import logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger('MetadataExtractorAgent')
    ```

## 7. MCP Integration
The `MetadataExtractorAgent` integrates with MCP via the `check_constraints` method, ensuring compliance with resource, temporal, and dependency constraints. The `MCPOrchestratorAgent` assigns tasks using a constraint satisfaction problem (CSP) solved with Google OR-Tools.

- **CSP Model**:
  - **Variables**: Extraction tasks (e.g., extract metadata for file X).
  - **Domains**: Available `MetadataExtractorAgent` instances.
  - **Constraints**:
    - Resource: CPU ≤ 1 core, memory ≤ 2GB.
    - Temporal: Complete within deadline.
    - Dependency: Wait for `ASTGenerated` event.
  - **Solver**: OR-Tools for optimal task assignment.

## 8. Testing and Validation
### 8.1 Unit Tests
```python
import unittest
from unittest.mock import MagicMock

class TestMetadataExtractorAgent(unittest.TestCase):
    def setUp(self):
        self.agent = MetadataExtractorAgent(kafka_bootstrap_servers='localhost:9092')

    def test_compute_metrics_python(self):
        ast = {'body': [{'type': 'FunctionDef'}]}
        content = 'def hello():\n    pass\n# Comment'
        metrics = self.agent.compute_metrics('Python', ast, content)
        self.assertEqual(metrics['lines_of_code'], 3)
        self.assertGreater(metrics['cyclomatic_complexity'], 0)
        self.assertGreater(metrics['comment_ratio'], 0)

    def test_extract_dependencies_python(self):
        ast = {'body': [{'type': 'Import', 'names': [{'name': 'os'}]}]}
        deps = self.agent.extract_dependencies('Python', ast)
        self.assertIn('os', deps)

    def test_constraints_violation(self):
        constraints = {
            'resource_limit': {'cpu': 0.5, 'memory': '1GB'},
            'deadline': (datetime.now() - timedelta(seconds=1)).isoformat()
        }
        satisfied, violations = self.agent.check_constraints(constraints, time.time())
        self.assertFalse(satisfied)
        self.assertIn('Deadline exceeded', violations)

if __name__ == '__main__':
    unittest.main()
```

### 8.2 Success Criteria
- Extract metadata for 100K lines in <30 seconds.
- 95% accuracy in dependency detection.
- 100% constraint satisfaction for hard constraints.
- Zero sensitive data in metadata output.

## 9. Deployment
- **Environment**: Docker container with Kubernetes for scaling.
- **Configuration**:
  - Image: Python 3.9 with required libraries.
  - Resources: 2GB RAM, 1 CPU core per instance.
  - Scaling: Minimum 2 instances, auto-scale based on Kafka topic backlog.
- **Monitoring**:
  - Expose health check endpoint for Kubernetes liveness probes.
  - Monitor CPU/memory via Prometheus.

## 10. Future Enhancements
- **Phase 2**: Support Java and C++ dependency analysis.
- **Phase 3**: Integrate with `CodeSmellAgent` for advanced metrics (e.g., maintainability index).
- **Phase 4**: Support custom metric plugins for enterprise use cases.

## 11. Conclusion
The `MetadataExtractorAgent` is a vital component for extracting actionable metadata, enabling AI-driven code analysis. Its Python implementation ensures scalability, constraint compliance, and integration with the agentic, MCP-driven architecture. The schemas, endpoints, and mini component diagram provide a clear foundation for the MVP, with extensibility for future phases.