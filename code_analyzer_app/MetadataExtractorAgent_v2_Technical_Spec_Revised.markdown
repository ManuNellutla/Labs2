# Technical Specification for MetadataExtractorAgent Implementation (Revised)

*Generated on August 07, 2025, at 11:12 AM EDT*

## 1. Overview

The `MetadataExtractorAgent` is an autonomous component in the AI-based code analyzer and modernization application, responsible for interfacing with a provided metadata extractor to process Abstract Syntax Trees (ASTs) generated by the `ParserAgent`. It extracts metadata (e.g., code metrics, dependencies) and prepares it for storage and AI-driven analysis (e.g., by `CodeSmellAgent` in Phase 3). The agent operates within the agentic architecture, coordinated by the `OrchestratorAgent` without Multi-Agent Constraint Programming (MCP) or assistant-triggered workflows, as per the review feedback. The agent is designed to be lightweight, leveraging a pre-built extractor for metadata processing.

### 1.1 Responsibilities
- **Metadata Extraction**: Use a provided extractor to compute metrics (e.g., lines of code, cyclomatic complexity, comment ratio) and identify dependencies (e.g., Python imports, JavaScript `require` statements).
- **Data Formatting**: Transform extractor outputs into a standardized schema for storage and downstream processing.
- **Event Publishing**: Notify downstream agents (e.g., `StorageAgent`, `CodeSmellAgent`) via a message bus (e.g., Kafka).
- **Error Handling**: Manage extractor errors, logging issues for debugging and compliance.
- **Deployment Flexibility**: Support both cloud and on-premises environments.

### 1.2 Constraints
- **Resource Constraints**:
  - Maximum 1GB RAM per extraction task.
  - Maximum 1 CPU core per task.
- **Temporal Constraints**:
  - Process metadata for 100K lines of code within 30 seconds (MVP target).
- **Dependency Constraints**:
  - Requires AST from `ParserAgent` via `ASTGenerated` event.
  - Must complete before AI analysis by `CodeSmellAgent` (Phase 3).
- **Security Constraints**:
  - Ensure no sensitive data is included in metadata.
  - Log all extraction events for auditing.

### 1.3 Integration Points
- **Input**: Receives AST and file metadata from `ParserAgent` via `ASTGenerated` event.
- **Output**: Produces metadata as a `MetadataExtracted` event.
- **Coordination**: Interacts with `OrchestratorAgent` for task assignment (no MCP).
- **Storage**: Sends metadata to `StorageAgent` for persistence in MongoDB.
- **Extractor**: Interfaces with a provided extractor (e.g., a library or system component) for metadata processing.

## 2. Data Schema

The schemas align with the BRD, simplified to remove MCP-specific fields.

### 2.1 Input Schema
Received via the `ASTGenerated` event:
```json
{
  "event_id": "UUID",
  "file_id": "UUID",
  "repository_id": "UUID",
  "language": "string",
  "ast": "json",
  "timestamp": "timestamp"
}
```

### 2.2 Output Schema
Published as the `MetadataExtracted` event:
```json
{
  "event_id": "UUID",
  "file_id": "UUID",
  "repository_id": "UUID",
  "metadata": {
    "language": "string",
    "metrics": {
      "lines_of_code": "integer",
      "cyclomatic_complexity": "integer",
      "comment_ratio": "float"
    },
    "dependencies": ["string"]
  },
  "error_message": "string | null",
  "timestamp": "timestamp"
}
```

### 2.3 Storage Schema
Stored via `StorageAgent` in MongoDB:
```json
{
  "metadata_id": "UUID",
  "file_id": "UUID",
  "repository_id": "UUID",
  "language": "string",
  "metrics": {
    "lines_of_code": "integer",
    "cyclomatic_complexity": "integer",
    "comment_ratio": "float"
  },
  "dependencies": ["string"],
  "created_at": "timestamp"
}
```

## 3. Endpoints

The `MetadataExtractorAgent` uses Kafka topics for internal communication, simplified without MCP constraints.

### 3.1 Extract Metadata
- **Topic**: `metadata.extract`
- **Request**:
  ```typescript
  interface ExtractMetadataRequest {
    event_id: string;
    file_id: string;
    repository_id: string;
    language: string;
    ast: object;
    timestamp: string;
  }
  ```
- **Response** (Published to `metadata.result`):
  ```typescript
  interface ExtractMetadataResponse {
    event_id: string;
    file_id: string;
    repository_id: string;
    metadata: {
      language: string;
      metrics: {
        lines_of_code: number;
        cyclomatic_complexity: number;
        comment_ratio: number;
      };
      dependencies: string[];
    };
    error_message?: string;
    timestamp: string;
  }
  ```

### 3.2 Health Check
- **Topic**: `metadata.health`
- **Request**:
  ```typescript
  interface HealthCheckRequest {
    agent_id: string;
  }
  ```
- **Response**:
  ```typescript
  interface HealthCheckResponse {
    agent_id: string;
    status: "healthy" | "unhealthy";
    resource_usage: { cpu: number; memory: string };
    active_tasks: number;
  }
  ```

## 4. Mini Component Diagram

The following diagram highlights the `MetadataExtractorAgent`’s interactions, reflecting the simplified architecture without MCP or assistant triggers.

```mermaid
classDiagram
    subgraph CoreAgents
        MetadataExtractorAgent
        OrchestratorAgent
        ParserAgent
        StorageAgent
        CodeSmellAgent
    end

    class MetadataExtractorAgent {
        +extractMetadata()
        +formatResults()
    }
    class OrchestratorAgent {
        +coordinateAgents()
    }
    class ParserAgent {
        +parseCode()
        +generateAST()
    }
    class StorageAgent {
        +storeData()
        +redactSensitiveData()
    }
    class CodeSmellAgent {
        +detectCodeSmells()
    }

    OrchestratorAgent --> MetadataExtractorAgent : Coordinates
    ParserAgent --> MetadataExtractorAgent : Sends ASTGenerated
    MetadataExtractorAgent --> CodeSmellAgent : Sends MetadataExtracted (Phase 3)
    MetadataExtractorAgent --> StorageAgent : Stores Metadata
    StorageAgent --> MongoDB : Stores
    MetadataExtractorAgent --> Kafka : Publishes to
    MetadataExtractorAgent --> ProvidedExtractor : Uses

    note for CodeSmellAgent "Phase 3"
    note for ProvidedExtractor "External Library/System"
```

**Diagram Explanation**:
- **Grouping**: Agents are grouped in a `CoreAgents` subgraph for brevity.
- **Focus**: Emphasizes `MetadataExtractorAgent`’s interactions:
  - Receives `ASTGenerated` from `ParserAgent`.
  - Uses a `ProvidedExtractor` (external library/system) for metadata extraction.
  - Publishes `MetadataExtracted` to `CodeSmellAgent` (Phase 3).
  - Stores metadata via `StorageAgent`.
  - Coordinated by `OrchestratorAgent` (no MCP).
- **External Systems**: Shows MongoDB, Kafka, and the `ProvidedExtractor`.
- **Phase Annotation**: Notes `CodeSmellAgent` as Phase 3 and `ProvidedExtractor` as external.

## 5. Python Implementation

The `MetadataExtractorAgent` is implemented in Python, assuming a provided extractor (e.g., a library like `astroid` for Python or `esprima` for JavaScript, or a system component). It acts as a wrapper, formatting results and publishing events.

### 5.1 Dependencies
```bash
pip install kafka-python psutil
# Provided extractor assumed to be installed/configured separately
```

### 5.2 Core Implementation
```python
import json
import time
import uuid
from kafka import KafkaConsumer, KafkaProducer
import psutil
from datetime import datetime
from typing import Dict, Optional, List

# Mock provided extractor (replace with actual library/system integration)
class ProvidedExtractor:
    def extract(self, language: str, ast: Dict, content: str = "") -> Dict:
        """Mock implementation of provided extractor."""
        # Simulate extraction (replace with actual extractor call)
        return {
            'metrics': {
                'lines_of_code': 100,  # Example
                'cyclomatic_complexity': 5,
                'comment_ratio': 0.2
            },
            'dependencies': ['os', 'sys'] if language == 'Python' else ['lodash']
        }

class MetadataExtractorAgent:
    def __init__(self, kafka_bootstrap_servers: str, extractor: ProvidedExtractor):
        self.producer = KafkaProducer(bootstrap_servers=kafka_bootstrap_servers)
        self.consumer = KafkaConsumer(
            'metadata.extract',
            bootstrap_servers=kafka_bootstrap_servers,
            group_id='metadata_agent_group'
        )
        self.extractor = extractor

    def process_ast(self, message: Dict):
        """Process AST using provided extractor and publish metadata."""
        event_id = str(uuid.uuid4())
        file_id = message['file_id']
        repository_id = message['repository_id']
        language = message['language']
        ast = message['ast']

        try:
            # Fetch content if needed (simplified, assume AST is sufficient)
            content = "mock_content"

            # Use provided extractor
            extracted_data = self.extractor.extract(language, ast, content)

            # Format response
            response = {
                'event_id': event_id,
                'file_id': file_id,
                'repository_id': repository_id,
                'metadata': {
                    'language': language,
                    'metrics': extracted_data['metrics'],
                    'dependencies': extracted_data['dependencies']
                },
                'error_message': None,
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            response = {
                'event_id': event_id,
                'file_id': file_id,
                'repository_id': repository_id,
                'metadata': None,
                'error_message': str(e),
                'timestamp': datetime.now().isoformat()
            }

        # Publish result
        self.producer.send('metadata.result', json.dumps(response).encode('utf-8'))

    def run(self):
        """Main loop to consume and process messages."""
        for message in self.consumer:
            try:
                msg = json.loads(message.value.decode('utf-8'))
                self.process_ast(msg)
            except Exception as e:
                print(f"Error processing message: {str(e)}")

    def health_check(self, agent_id: str) -> Dict:
        """Return health status and resource usage."""
        process = psutil.Process()
        return {
            'agent_id': agent_id,
            'status': 'healthy',
            'resource_usage': {
                'cpu': process.cpu_percent(interval=0.1) / psutil.cpu_count(),
                'memory': f"{process.memory_info().rss / 1e9}GB"
            },
            'active_tasks': len(self.consumer.assignment())
        }
```

### 5.3 Configuration
- **Kafka Topics**:
  - Subscribe to: `metadata.extract`
  - Publish to: `metadata.result`, `metadata.health`
- **Environment Variables**:
  - `KAFKA_BOOTSTRAP_SERVERS`: Kafka server address (e.g., `localhost:9092`).
  - `EXTRACTOR_CONFIG`: Path to extractor configuration (if applicable).
- **Extractor**: Assumed to be configured externally (e.g., via system settings or library initialization).

## 6. Error Handling and Logging
- **Error Handling**:
  - Catch extractor exceptions and include in `error_message`.
  - Retry transient failures (e.g., Kafka publish errors) up to 3 times.
- **Logging**:
  - Log events to `AuditLog` via `ComplianceAgent`:
    ```json
    {
      "event_id": "UUID",
      "type": "metadata_extract",
      "timestamp": "timestamp",
      "user_id": "UUID",
      "details": "Extracted metadata for file {file_id}",
      "compliance_status": "compliant"
    }
    ```
  - Use Python’s `logging` module:
    ```python
    import logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger('MetadataExtractorAgent')
    ```

## 7. Integration with Orchestrator
The `MetadataExtractorAgent` integrates with the `OrchestratorAgent` for task assignment, without MCP or assistant triggers. The orchestrator assigns tasks based on a simple queue, ensuring dependencies (e.g., `ASTGenerated` event) are met.

## 8. Testing and Validation
### 8.1 Unit Tests
```python
import unittest
from unittest.mock import MagicMock

class TestMetadataExtractorAgent(unittest.TestCase):
    def setUp(self):
        self.extractor = MagicMock()
        self.extractor.extract.return_value = {
            'metrics': {'lines_of_code': 100, 'cyclomatic_complexity': 5, 'comment_ratio': 0.2},
            'dependencies': ['os']
        }
        self.agent = MetadataExtractorAgent(kafka_bootstrap_servers='localhost:9092', extractor=self.extractor)

    def test_process_ast(self):
        message = {
            'event_id': str(uuid.uuid4()),
            'file_id': str(uuid.uuid4()),
            'repository_id': str(uuid.uuid4()),
            'language': 'Python',
            'ast': {'body': []}
        }
        self.agent.process_ast(message)
        self.assertTrue(self.extractor.extract.called)

    def test_health_check(self):
        result = self.agent.health_check('agent_1')
        self.assertEqual(result['status'], 'healthy')
        self.assertIn('resource_usage', result)

if __name__ == '__main__':
    unittest.main()
```

### 8.2 Success Criteria
- Extract metadata for 100K lines in <30 seconds.
- 95% accuracy in dependency and metric extraction (assuming extractor quality).
- Zero sensitive data in metadata output.

## 9. Deployment
- **Environment**: Docker container with Kubernetes for scaling.
- **Configuration**:
  - Image: Python 3.9 with minimal libraries.
  - Resources: 1GB RAM, 1 CPU core per instance.
  - Scaling: Minimum 2 instances, auto-scale based on Kafka topic backlog.
- **Monitoring**:
  - Expose health check endpoint for Kubernetes liveness probes.
  - Monitor CPU/memory via Prometheus.

## 10. Future Enhancements
- **Phase 2**: Support Java and C++ via extractor updates.
- **Phase 3**: Integrate with `CodeSmellAgent` for advanced metrics.
- **Phase 4**: Support custom extractor configurations for enterprise use.

## 11. Conclusion
The `MetadataExtractorAgent` is a lightweight component that interfaces with a provided extractor, ensuring seamless metadata processing in the agentic architecture. Its simplified design, without MCP or assistant triggers, aligns with the review feedback while maintaining scalability and flexibility for cloud and on-premises deployments. The schemas, endpoints, and mini component diagram provide a robust foundation for the MVP.